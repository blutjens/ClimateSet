model:
  loss_function: climax_lon_lat_rmse
  monitor: val/llrmse_climax
  super_emulation: false
  super_decoder: false
  optimizer:
    name: adam
    is_filtered: false
    lr: 0.0002
    weight_decay: 1.0e-06
    eps: 1.0e-08
  _target_: emulator.src.core.models.baselines.UNet
  in_var_ids: ${datamodule.in_var_ids}
  out_var_ids: ${datamodule.out_var_ids}
  seq_to_seq: ${datamodule.seq_to_seq}
  seq_len: ${datamodule.seq_len}
  activation_function: null
  encoder_name: vgg11
  channels_last: ${datamodule.channels_last}
  scheduler:
    _target_: torch.optim.lr_scheduler.ExponentialLR
    gamma: 0.98
datamodule:
  batch_size: 4
  _target_: emulator.src.datamodules.climate_datamodule.ClimateDataModule
  in_var_ids:
  - BC_sum
  - CO2_sum
  - SO2_sum
  - CH4_sum
  out_var_ids:
  - pr
  - tas
  seq_to_seq: true
  channels_last: false
  eval_batch_size: 4
  train_historical_years: 1850-1900
  test_years: 2015-2100
  train_years: 2015-2100
  val_split: 0.1
  train_scenarios:
  - ssp126
  - ssp370
  - ssp585
  test_scenarios:
  - ssp245
  train_models:
  - NorESM2-LM
  num_ensembles: 1
  num_workers: 0
  pin_memory: false
  load_train_into_mem: true
  load_test_into_mem: true
  verbose: true
  seed: 11
  seq_len: 12
  lon: 96
  lat: 144
  num_levels: 1
work_dir: ${hydra:runtime.cwd}
ckpt_dir: ${work_dir}/checkpoints/
log_dir: ${work_dir}/logs/
print_config: true
ignore_warnings: true
test_after_training: true
save_config_to_wandb: true
verbose: true
seed: 42
name: default
trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: auto
  min_epochs: 1
  max_epochs: 1
  gradient_clip_val: 1.0
  num_sanity_val_steps: 0
callbacks: {}
logger:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    name: ${name}
    tags: []
    notes: '...'
    project: emulator
    group: causalpaca
    resume: allow
    reinit: true
    mode: online
    save_dir: ${work_dir}/
    offline: false
    id: null
    log_model: true
    prefix: ''
    job_type: train
mode: {}
local: {}
